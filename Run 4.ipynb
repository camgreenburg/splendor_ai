{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from game import Game\n",
    "from ai import SplendorAI\n",
    "from player import get_phase_parameters\n",
    "from constants import *\n",
    "from datetime import datetime\n",
    "from player import get_phase_parameters\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simply used to initialize players\n",
    "base_game = Game(id=0, n_players=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 15644341988200897924\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 9854012621\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 9750500025190935530\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temperature will decrease slightly quicker over time\n",
    "def calculate_temperature(round):\n",
    "    return 2/(1+round/1.2) * 1/(1+round/8) * 1/(1+round/20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "players = base_game.players\n",
    "game_data = defaultdict(list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test multiple runs of games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ON ROUND 0\n",
      "........................................................................................................................training win model\n",
      "training Q1 model\n",
      "training Q3 model\n",
      "training Q5 model\n",
      "training win model\n",
      "training Q1 model\n",
      "training Q3 model\n",
      "training Q5 model\n",
      "training win model\n",
      "training Q1 model\n",
      "training Q3 model\n",
      "training Q5 model\n",
      "training win model\n",
      "training Q1 model\n",
      "training Q3 model\n",
      "training Q5 model\n",
      "/\n",
      "R/S 0/0 AVERAGE GAME LENGTH:  149.36666666666667\n",
      "R/S 0/0 AVERAGE CARDS PURCHASED:  13.685416666666667\n",
      "........................................................................................................................training win model\n",
      "training Q1 model\n",
      "training Q3 model\n",
      "training Q5 model\n",
      "training win model\n",
      "training Q1 model\n",
      "training Q3 model\n",
      "training Q5 model\n",
      "training win model\n",
      "training Q1 model\n",
      "training Q3 model\n",
      "training Q5 model\n",
      "training win model\n",
      "training Q1 model\n",
      "training Q3 model\n",
      "training Q5 model\n",
      "/\n",
      "R/S 0/1 AVERAGE GAME LENGTH:  129.3\n",
      "R/S 0/1 AVERAGE CARDS PURCHASED:  13.5\n",
      "saving run4_player_0_round__win_0.h5\n",
      "saving run4_player_0_round__Q1_0.h5\n",
      "saving run4_player_0_round__Q3_0.h5\n",
      "saving run4_player_0_round__Q5_0.h5\n",
      "saving run4_player_1_round__win_0.h5\n",
      "saving run4_player_1_round__Q1_0.h5\n",
      "saving run4_player_1_round__Q3_0.h5\n",
      "saving run4_player_1_round__Q5_0.h5\n",
      "saving run4_player_2_round__win_0.h5\n",
      "saving run4_player_2_round__Q1_0.h5\n",
      "saving run4_player_2_round__Q3_0.h5\n",
      "saving run4_player_2_round__Q5_0.h5\n",
      "saving run4_player_3_round__win_0.h5\n",
      "saving run4_player_3_round__Q1_0.h5\n",
      "saving run4_player_3_round__Q3_0.h5\n",
      "saving run4_player_3_round__Q5_0.h5\n",
      "ON ROUND 1\n",
      "........................................................................................................................training win model\n",
      "training Q1 model\n",
      "training Q3 model\n",
      "training Q5 model\n",
      "training win model\n",
      "training Q1 model\n",
      "training Q3 model\n",
      "training Q5 model\n",
      "training win model\n",
      "training Q1 model\n",
      "training Q3 model\n",
      "training Q5 model\n",
      "training win model\n",
      "training Q1 model\n",
      "training Q3 model\n",
      "training Q5 model\n",
      "/\n",
      "R/S 1/0 AVERAGE GAME LENGTH:  130.76666666666668\n",
      "R/S 1/0 AVERAGE CARDS PURCHASED:  13.622916666666667\n",
      "........................................................................................................................training win model\n",
      "training Q1 model\n",
      "training Q3 model\n",
      "training Q5 model\n",
      "training win model\n",
      "training Q1 model\n",
      "training Q3 model\n",
      "training Q5 model\n",
      "training win model\n",
      "training Q1 model\n",
      "training Q3 model\n",
      "training Q5 model\n",
      "training win model\n",
      "training Q1 model\n",
      "training Q3 model\n",
      "training Q5 model\n",
      "/\n",
      "R/S 1/1 AVERAGE GAME LENGTH:  130.6\n",
      "R/S 1/1 AVERAGE CARDS PURCHASED:  13.5375\n",
      "saving run4_player_0_round__win_1.h5\n",
      "saving run4_player_0_round__Q1_1.h5\n",
      "saving run4_player_0_round__Q3_1.h5\n",
      "saving run4_player_0_round__Q5_1.h5\n",
      "saving run4_player_1_round__win_1.h5\n",
      "saving run4_player_1_round__Q1_1.h5\n",
      "saving run4_player_1_round__Q3_1.h5\n",
      "saving run4_player_1_round__Q5_1.h5\n",
      "saving run4_player_2_round__win_1.h5\n",
      "saving run4_player_2_round__Q1_1.h5\n",
      "saving run4_player_2_round__Q3_1.h5\n",
      "saving run4_player_2_round__Q5_1.h5\n",
      "saving run4_player_3_round__win_1.h5\n",
      "saving run4_player_3_round__Q1_1.h5\n",
      "saving run4_player_3_round__Q3_1.h5\n",
      "saving run4_player_3_round__Q5_1.h5\n",
      "ON ROUND 2\n",
      "........................................................................................................................training win model\n",
      "training Q1 model\n",
      "training Q3 model\n",
      "training Q5 model\n",
      "training win model\n",
      "training Q1 model\n",
      "training Q3 model\n",
      "training Q5 model\n",
      "training win model\n",
      "training Q1 model\n",
      "training Q3 model\n",
      "training Q5 model\n",
      "training win model\n",
      "training Q1 model\n",
      "training Q3 model\n",
      "training Q5 model\n",
      "/\n",
      "R/S 2/0 AVERAGE GAME LENGTH:  123.7\n",
      "R/S 2/0 AVERAGE CARDS PURCHASED:  13.75625\n",
      "........................................................................................................................training win model\n",
      "training Q1 model\n",
      "training Q3 model\n",
      "training Q5 model\n",
      "training win model\n",
      "training Q1 model\n",
      "training Q3 model\n",
      "training Q5 model\n",
      "training win model\n",
      "training Q1 model\n",
      "training Q3 model\n",
      "training Q5 model\n",
      "training win model\n",
      "training Q1 model\n",
      "training Q3 model\n",
      "training Q5 model\n",
      "/\n",
      "R/S 2/1 AVERAGE GAME LENGTH:  123.6\n",
      "R/S 2/1 AVERAGE CARDS PURCHASED:  13.729166666666666\n",
      "saving run4_player_0_round__win_2.h5\n",
      "saving run4_player_0_round__Q1_2.h5\n",
      "saving run4_player_0_round__Q3_2.h5\n",
      "saving run4_player_0_round__Q5_2.h5\n",
      "saving run4_player_1_round__win_2.h5\n",
      "saving run4_player_1_round__Q1_2.h5\n",
      "saving run4_player_1_round__Q3_2.h5\n",
      "saving run4_player_1_round__Q5_2.h5\n",
      "saving run4_player_2_round__win_2.h5\n",
      "saving run4_player_2_round__Q1_2.h5\n",
      "saving run4_player_2_round__Q3_2.h5\n",
      "saving run4_player_2_round__Q5_2.h5\n",
      "saving run4_player_3_round__win_2.h5\n",
      "saving run4_player_3_round__Q1_2.h5\n",
      "saving run4_player_3_round__Q3_2.h5\n",
      "saving run4_player_3_round__Q5_2.h5\n",
      "ON ROUND 3\n",
      "......................................"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-7eae13f18504>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_simulations_per_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mnew_game\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m200\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplayers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mplayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m             \u001b[0mnew_game\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m             \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/ntfsl/workspace/splendor_ai/game.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m                                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mturn\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                                 \u001b[0mplayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactive_turn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m                                 \u001b[0mplayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake_turn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m                                 \u001b[0mplayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactive_turn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m                                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_plain_history\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/ntfsl/workspace/splendor_ai/player.py\u001b[0m in \u001b[0;36mtake_turn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    180\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_q_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_turn_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobjective_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/ntfsl/workspace/splendor_ai/player.py\u001b[0m in \u001b[0;36mmake_turn_action\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    287\u001b[0m                 \u001b[0mpurchasing_options\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimulate_purchasing_options\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m                 \u001b[0mreserving_options\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimulate_reserving_options\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m                 \u001b[0mgem_taking_options\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimulate_gem_taking_options\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m                 \u001b[0;31m# now determine best course of action based on q-weighting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/ntfsl/workspace/splendor_ai/player.py\u001b[0m in \u001b[0;36msimulate_gem_taking_options\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    481\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgem_combinations\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m                         \u001b[0mgem_serializations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_serializations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgem_changes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgem_combinations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 483\u001b[0;31m                         \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_predictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgem_serializations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    484\u001b[0m \t\t\treturn {\n\u001b[1;32m    485\u001b[0m                                 \u001b[0;34m'predictions'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/ntfsl/workspace/splendor_ai/ai.py\u001b[0m in \u001b[0;36mmake_predictions\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    263\u001b[0m                 \u001b[0mstacked_network_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_array\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0minput_array\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0munstacked_network_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m                 \u001b[0mwin_prediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwin_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstacked_network_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m                 \u001b[0mq_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_networks\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1833\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1834\u001b[0m         return self._predict_loop(f, ins, batch_size=batch_size,\n\u001b[0;32m-> 1835\u001b[0;31m                                   verbose=verbose, steps=steps)\n\u001b[0m\u001b[1;32m   1836\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1837\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_predict_loop\u001b[0;34m(self, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1328\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1330\u001b[0;31m                 \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1331\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1332\u001b[0m                     \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2476\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2478\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2479\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1140\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1141\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1321\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1310\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1311\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1312\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1418\u001b[0m         return tf_session.TF_Run(\n\u001b[1;32m   1419\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1420\u001b[0;31m             status, run_metadata)\n\u001b[0m\u001b[1;32m   1421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1422\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_rounds = 20\n",
    "n_sets_per_round = 2\n",
    "n_simulations_per_set = 120\n",
    "set_durations = {}\n",
    "start_time = datetime.now()\n",
    "for i in range(n_rounds):\n",
    "    print('ON ROUND', i)\n",
    "    for j in range(n_sets_per_round):\n",
    "        set_start_time = datetime.now()\n",
    "        for k in range(n_simulations_per_set):\n",
    "            new_game = Game(id=i*200 + j*100+k, players=players)\n",
    "            new_game.run()    \n",
    "            sys.stdout.write('.')\n",
    "            sys.stdout.flush()\n",
    "            # record historic game data for each set\n",
    "            game_data[(i,j)].append(new_game.copy_plain_data_for_self_and_players())\n",
    "        set_stop_time = datetime.now()\n",
    "        duration = (set_start_time-set_stop_time).seconds\n",
    "        for player in players:\n",
    "            player.transfer_history_to_ai()\n",
    "            player.ai.train_models(verbose=0, n_epochs=12)\n",
    "        set_durations[(i,j,k)] = duration\n",
    "        print('/')\n",
    "        sys.stdout.flush()\n",
    "        avg_game_length = np.mean([x['game']['turn'] for x in game_data[(i,j)]])\n",
    "        print('R/S %d/%d AVERAGE GAME LENGTH: ' % (i, j), avg_game_length)\n",
    "        avg_cards_purchased = np.mean([\n",
    "            [\n",
    "                pdata['n_cards'] \n",
    "                for pdata in x['players'].values()\n",
    "            ] \n",
    "            for x in game_data[(i,j)]\n",
    "        ]\n",
    "        )\n",
    "        print('R/S %d/%d AVERAGE CARDS PURCHASED: ' % (i, j), avg_cards_purchased)\n",
    "    for player in players:\n",
    "        player.reset(reset_extended_history=True)\n",
    "    phase = min(5, i // 4 + 1)\n",
    "    temperature = calculate_temperature(i)\n",
    "    \n",
    "    for p_i, player in enumerate(players):\n",
    "        player.temperature = temperature\n",
    "        player.decision_weighting = get_phase_parameters(phase)\n",
    "        model_name = 'run4_player_%s_round_' % str(p_i)\n",
    "        player.ai.save_models(model_name, index=i)\n",
    "    \n",
    "    \n",
    "\n",
    "for time in [start_time, stop_time]:\n",
    "    print(time.strftime('%x %X'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9338"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Mapping, Container \n",
    "from sys import getsizeof\n",
    "\n",
    "def deep_getsizeof(o, ids): \n",
    "\n",
    "    d = deep_getsizeof\n",
    "    if id(o) in ids:\n",
    "        return 0\n",
    "\n",
    "    r = getsizeof(o)\n",
    "    ids.add(id(o))\n",
    "\n",
    "    if isinstance(o, str) or isinstance(0, bytes):\n",
    "        return r\n",
    "    \n",
    "    if isinstance(o, np.ndarray):\n",
    "        return r\n",
    "\n",
    "    if isinstance(o, Mapping):\n",
    "        return r + sum(d(k, ids) + d(v, ids) for k, v in o.items())\n",
    "\n",
    "    if isinstance(o, Container):\n",
    "        return r + sum(d(x, ids) for x in o)\n",
    "\n",
    "    return r\n",
    "\n",
    "deep_getsizeof(players[0].extended_serialized_action_history[0], set())\n",
    "\n",
    "# for k in list(locals().keys()):\n",
    "#    v = locals()[k]\n",
    "#    size = deep_getsizeof(v, set())\n",
    "#    if size > 100000:\n",
    "#        print(k, ':', size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__dict__ 28\n",
      "decision_weighting 4\n",
      "extended_lagged_q_state_history 0\n",
      "extended_output 0\n",
      "extended_plain_action_history 0\n",
      "extended_serialized_action_history 0\n",
      "lagged_q_state_history 0\n",
      "n_reserved_cards_tiers 3\n",
      "objectives 0\n",
      "other_players 3\n",
      "owned_cards 0\n",
      "plain_action_history 0\n",
      "q_state_history 0\n",
      "reserved_cards 0\n",
      "serialized_action_history 0\n"
     ]
    }
   ],
   "source": [
    "players[1].reset(reset_extended_history=True)\n",
    "for k in dir(players[1]):\n",
    "    v = getattr(players[1], k)\n",
    "    if isinstance(v, (list, dict)):\n",
    "        print(k, len(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_game = Game(id=i*200 + j*100+1.1, players=players)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__dict__ 28\n",
      "decision_weighting 4\n",
      "extended_lagged_q_state_history 1167\n",
      "extended_output 0\n",
      "extended_plain_action_history 0\n",
      "extended_serialized_action_history 1167\n",
      "lagged_q_state_history 0\n",
      "n_reserved_cards_tiers 3\n",
      "objectives 0\n",
      "other_players 3\n",
      "owned_cards 0\n",
      "plain_action_history 0\n",
      "q_state_history 0\n",
      "reserved_cards 0\n",
      "serialized_action_history 0\n"
     ]
    }
   ],
   "source": [
    "for k in dir(players[2]):\n",
    "    v = getattr(players[2], k)\n",
    "    if isinstance(v, (list, dict)):\n",
    "        print(k, len(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7479"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(players[0].ai.extended_serialized_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_game.available_tier_3_cards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_game.serialize()['available_cards']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[sum([card['points'] for card in player.owned_cards]) for player in players]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[sum([card['points'] for card in player.reserved_cards if card['tier']==3]) for player in players]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[player.points for player in players]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[player.gems  for player in players]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_game.gems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[player.active_turn for player in players]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_durations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "players[2].simulate_purchasing_options()['score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "players[2].simulate_gem_taking_options()['score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looks like i had an oversight in the decisionmaking code when temperatures were higher\n",
    "for z in range(10000):\n",
    "    if players[2].make_choice([13.436, -1e100, 11.29427]) == 1:\n",
    "        print('uh oh')\n",
    "        print(z)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.asarray([13.436, -1e100, 11.29427])\n",
    "arr2 = np.exp(arr/players[2].temperature)\n",
    "print(arr2)\n",
    "print(arr2/np.sum(arr2)==0.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120\n",
      "120\n",
      "120\n",
      "38\n",
      "120\n",
      "120\n",
      "120\n"
     ]
    }
   ],
   "source": [
    "for k, v in game_data.items():\n",
    "    print(len(v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTES\n",
    "\n",
    "There was a memory leak with non-extended history!!! the `Game.reset()` method should reset this history each game, but for some reason it wasn't here...\n",
    "\n",
    "Game must've taken forever..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
